{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "5b747606",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "with open('isolet1+2+3+4.data/isolet1+2+3+4.data') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        splitted = line.split(', ')\n",
    "        label = float(splitted[-1])\n",
    "        features = [float(x) for x in splitted[:-1]]\n",
    "        x_train.append(features)\n",
    "        y_train.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "69f10109",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "y_test = []\n",
    "with open('isolet5.data/isolet5.data') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        splitted = line.split(', ')\n",
    "        label = float(splitted[-1])\n",
    "        features = [float(x) for x in splitted[:-1]]\n",
    "        x_test.append(features)\n",
    "        y_test.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "cf69219d-88ee-45da-8f51-d2cd6962d07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6238"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "60bb4b1f-04f4-4180-8bfe-b8d5274041f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class hd_classifier():\n",
    " \n",
    "        \n",
    "    \n",
    "    def __init__(self, dim=5000):\n",
    "\n",
    "        self.x_train = np.asarray(x_train)\n",
    "        self.y_train = np.asarray(y_train)\n",
    "\n",
    "        self.max_x_train = 1\n",
    "        self.min_x_train = -1\n",
    "        self.feature_len = len(self.x_train[0])\n",
    "        self.label_len = 26\n",
    "        self.dim = dim\n",
    "        self.gamma = 0.5\n",
    "        self.level_hv_min = self.random_hv_generator()\n",
    "        self.level_hv_max = self.random_hv_generator()\n",
    "        \n",
    "        # initializing middle level-hypervectors\n",
    "        self.level_hvs = [None for i in range(9)]\n",
    "        for i in range(len(self.level_hvs)):\n",
    "            self.level_hvs[i] = [0 for i in range(self.dim)]\n",
    "            ratio = (i+1)/10\n",
    "            for j in range(self.dim):\n",
    "                if random.uniform(0, 1) < ratio:\n",
    "                    self.level_hvs[i][j] = self.level_hv_max[j]\n",
    "                else:\n",
    "                    self.level_hvs[i][j] = self.level_hv_min[j]\n",
    "\n",
    "    \n",
    "        # generate identification vectors for association-based encoding\n",
    "        self.id_assoc = [None for i in range(self.feature_len)]\n",
    "        for i in range(len(self.id_assoc)):\n",
    "            self.id_assoc[i] = self.random_hv_generator()\n",
    "            \n",
    "        # calc gaussian kernel matrix\n",
    "        mu = 0\n",
    "        sigma = 1\n",
    "        self.kernel = np.asarray([[np.random.normal(mu, sigma, 1)[0] for i in range(self.feature_len)] for j in range(self.dim)])\n",
    "\n",
    "        # bundled classifier hv for every label in perm-based encoding\n",
    "        self.classes_perm_encodings = np.asarray([[0 for i in range(self.dim)] for j in range(self.label_len)])\n",
    "\n",
    "        # bundled classifier hv for every label in assoc-based encoding\n",
    "        self.classes_assoc_encodings = np.asarray([[0 for i in range(self.dim)] for j in range(self.label_len)])\n",
    "\n",
    "        # bundled classifier hv for every label in kernel-based encoding\n",
    "        self.classes_kernel_encodings = np.asarray([[0 for i in range(self.dim)] for j in range(self.label_len)])\n",
    "    \n",
    "\n",
    "    def random_hv_generator(self):\n",
    "        hv = np.asarray([1 for i in range(self.dim)])\n",
    "        for i in range(self.dim):\n",
    "            if random.randint(0, 1) == 0:\n",
    "                hv[i] = -1\n",
    "        return hv\n",
    "\n",
    "    def perm_hv(self, hv, perm_num):\n",
    "        return np.roll(hv, perm_num)\n",
    "    \n",
    "    def assign_level_hv(self, e):\n",
    "        result = None\n",
    "        ratio = ((e+1)/2)*10\n",
    "        \n",
    "        if 0 <= ratio < 0.5:\n",
    "            result = self.level_hv_min\n",
    "        elif 0.5 <= ratio < 1.5:\n",
    "            result = self.level_hvs[0]\n",
    "        elif 1.5 <= ratio < 2.5:\n",
    "            result = self.level_hvs[1]\n",
    "        elif 2.5 <= ratio < 3.5:\n",
    "            result = self.level_hvs[2]\n",
    "        elif 3.5 <= ratio < 4.5:\n",
    "            result = self.level_hvs[3]\n",
    "        elif 4.5 <= ratio < 5.5:\n",
    "            result = self.level_hvs[4]\n",
    "        elif 5.5 <= ratio < 6.5:\n",
    "            result = self.level_hvs[5]\n",
    "        elif 6.5 <= ratio < 7.5:\n",
    "            result = self.level_hvs[6]\n",
    "        elif 7.5 <= ratio < 8.5:\n",
    "            result = self.level_hvs[7]\n",
    "        elif 8.5 <= ratio < 9.5:\n",
    "            result = self.level_hvs[8]\n",
    "        else:\n",
    "            result = self.level_hv_max\n",
    "        return result\n",
    "\n",
    "    def calc_perm_enc(self, sample):\n",
    "        ratio = 0\n",
    "        encoded_sample = np.asarray([0 for i in range(self.dim)])\n",
    "        for i, e in np.ndenumerate(sample):\n",
    "            feature_hv = self.assign_level_hv(e)\n",
    "            feature_hv = self.perm_hv(feature_hv, i[0])\n",
    "            encoded_sample = np.add(encoded_sample, feature_hv)\n",
    "\n",
    "        return encoded_sample\n",
    "    \n",
    "    \n",
    "    def FV_perm_classifier(self):\n",
    "        # encoding each feature and then bundling their permutations\n",
    "        zipped_x_y_train = np.column_stack((self.x_train, self.y_train))\n",
    "\n",
    "        for e in zipped_x_y_train:\n",
    "            label = int(e[self.feature_len])\n",
    "            sample = np.asarray(e[:self.feature_len])\n",
    "            idx = label - 1\n",
    "            self.classes_perm_encodings[idx] = np.add(self.classes_perm_encodings[idx], self.calc_perm_enc(sample))\n",
    "        return\n",
    "    \n",
    "    def cosine(self, encoded_sample, classes):\n",
    "        pred_idx = 0\n",
    "        cosine_val = 0\n",
    "        for i in range(self.label_len):\n",
    "            sim_val = np.dot(encoded_sample, classes[i])\n",
    "            if sim_val > cosine_val:\n",
    "                pred_idx = i\n",
    "                cosine_val = sim_val\n",
    "\n",
    "        return (pred_idx + 1)\n",
    "    \n",
    "    \n",
    "    def predict_perm(self, sample):\n",
    "        perm_encoded_sample = self.calc_perm_enc(sample)\n",
    "        predicted_label = self.cosine(perm_encoded_sample, self.classes_perm_encodings)\n",
    "        return predicted_label\n",
    "    \n",
    "    \n",
    "    def FV_adaptive_perm_classifier(self):\n",
    "        # encoding each feature and then bundling their permutations\n",
    "        zipped_x_y_train = np.column_stack((self.x_train, self.y_train))\n",
    "\n",
    "        for e in zipped_x_y_train:\n",
    "            label = int(e[self.feature_len])\n",
    "            sample = np.asarray(e[:self.feature_len])\n",
    "            encoded_sample = self.calc_perm_enc(sample)\n",
    "            # Note: predict methods get sample as input, not encoded sample\n",
    "            pred_label = self.predict_perm(sample)\n",
    "\n",
    "            label_idx = label - 1\n",
    "            pred_label_idx = pred_label - 1\n",
    "            \n",
    "            #adaptive single-pass training\n",
    "            # adding to the correct label class\n",
    "            dot_sample_class = np.dot(encoded_sample, self.classes_perm_encodings[label_idx])\n",
    "            norm_sample_class_mult = np.linalg.norm(encoded_sample)*np.linalg.norm(self.classes_perm_encodings[label_idx])\n",
    "            cos_sim = dot_sample_class/norm_sample_class_mult if norm_sample_class_mult != 0 else 0\n",
    "            add_val = self.gamma*(1 - cos_sim)*encoded_sample\n",
    "            self.classes_perm_encodings[label_idx] = np.add(self.classes_perm_encodings[label_idx], add_val)\n",
    "            \n",
    "            # subtracting from wrong class\n",
    "            if label != pred_label:\n",
    "                dot_sample_pred_class = np.dot(encoded_sample, self.classes_perm_encodings[pred_label_idx])\n",
    "                norm_sample_pred_class = np.linalg.norm(encoded_sample)*np.linalg.norm(self.classes_perm_encodings[pred_label_idx])\n",
    "                sub_cos_sim = dot_sample_pred_class/norm_sample_pred_class if norm_sample_pred_class != 0 else 0\n",
    "                sub_val = self.gamma*(1 - sub_cos_sim)*encoded_sample\n",
    "                self.classes_perm_encodings[pred_label_idx] = np.subtract(self.classes_perm_encodings[pred_label_idx], sub_val)\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def FV_adaptive_assoc_classifier(self):\n",
    "        # encoding each feature and then bundling their associations\n",
    "        zipped_x_y_train = np.column_stack((self.x_train, self.y_train))\n",
    "\n",
    "        for e in zipped_x_y_train:\n",
    "            label = int(e[self.feature_len])\n",
    "            sample = np.asarray(e[:self.feature_len])\n",
    "            encoded_sample = self.calc_assoc_enc(sample)\n",
    "            # Note: predict methods get sample as input, not encoded sample\n",
    "            pred_label = self.predict_assoc(sample)\n",
    "\n",
    "            label_idx = label - 1\n",
    "            pred_label_idx = pred_label - 1\n",
    "            \n",
    "            #adaptive single-pass training\n",
    "            # adding to the correct label class\n",
    "            dot_sample_class = np.dot(encoded_sample, self.classes_assoc_encodings[label_idx])\n",
    "            norm_sample_class_mult = np.linalg.norm(encoded_sample)*np.linalg.norm(self.classes_assoc_encodings[label_idx])\n",
    "            cos_sim = dot_sample_class/norm_sample_class_mult if norm_sample_class_mult != 0 else 0\n",
    "            add_val = self.gamma*(1 - cos_sim)*encoded_sample\n",
    "            self.classes_assoc_encodings[label_idx] = np.add(self.classes_assoc_encodings[label_idx], add_val)\n",
    "            \n",
    "            # subtracting from wrong class\n",
    "            if label != pred_label:\n",
    "                dot_sample_pred_class = np.dot(encoded_sample, self.classes_assoc_encodings[pred_label_idx])\n",
    "                norm_sample_pred_class = np.linalg.norm(encoded_sample)*np.linalg.norm(self.classes_assoc_encodings[pred_label_idx])\n",
    "                sub_cos_sim = dot_sample_pred_class/norm_sample_pred_class if norm_sample_pred_class != 0 else 0\n",
    "                sub_val = self.gamma*(1 - sub_cos_sim)*encoded_sample\n",
    "                self.classes_assoc_encodings[pred_label_idx] = np.subtract(self.classes_assoc_encodings[pred_label_idx], sub_val)\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def FV_adaptive_kernel_classifier(self):\n",
    "        # encoding each feature and then bundling their kernel projection\n",
    "        zipped_x_y_train = np.column_stack((self.x_train, self.y_train))\n",
    "\n",
    "        for e in zipped_x_y_train:\n",
    "            label = int(e[self.feature_len])\n",
    "            sample = np.asarray(e[:self.feature_len])\n",
    "            encoded_sample = self.calc_kernel_enc(sample)\n",
    "            # Note: predict methods get sample as input, not encoded sample\n",
    "            pred_label = self.predict_kernel(sample)\n",
    "\n",
    "            label_idx = label - 1\n",
    "            pred_label_idx = pred_label - 1\n",
    "            \n",
    "            #adaptive single-pass training\n",
    "            # adding to the correct label class\n",
    "            dot_sample_class = np.dot(encoded_sample, self.classes_kernel_encodings[label_idx])\n",
    "            norm_sample_class_mult = np.linalg.norm(encoded_sample)*np.linalg.norm(self.classes_kernel_encodings[label_idx])\n",
    "            cos_sim = dot_sample_class/norm_sample_class_mult if norm_sample_class_mult != 0 else 0\n",
    "            add_val = self.gamma*(1 - cos_sim)*encoded_sample\n",
    "            self.classes_kernel_encodings[label_idx] = np.add(self.classes_kernel_encodings[label_idx], add_val)\n",
    "            \n",
    "            # subtracting from wrong class\n",
    "            if label != pred_label:\n",
    "                dot_sample_pred_class = np.dot(encoded_sample, self.classes_kernel_encodings[pred_label_idx])\n",
    "                norm_sample_pred_class = np.linalg.norm(encoded_sample)*np.linalg.norm(self.classes_kernel_encodings[pred_label_idx])\n",
    "                sub_cos_sim = dot_sample_pred_class/norm_sample_pred_class if norm_sample_pred_class != 0 else 0\n",
    "                sub_val = self.gamma*(1 - sub_cos_sim)*encoded_sample\n",
    "                self.classes_kernel_encodings[pred_label_idx] = np.subtract(self.classes_kernel_encodings[pred_label_idx], sub_val)\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def calc_assoc_enc(self, sample):\n",
    "        ratio = 0\n",
    "        encoded_sample = np.asarray([0 for i in range(self.dim)])\n",
    "\n",
    "        for i, e in np.ndenumerate(sample):\n",
    "            feature_idx = i[0]\n",
    "            id_hv = self.id_assoc[feature_idx]\n",
    "            feature_hv = self.assign_level_hv(e)\n",
    "            feature_hv = np.multiply(feature_hv, id_hv)\n",
    "            encoded_sample = np.add(encoded_sample, feature_hv)\n",
    "\n",
    "        return encoded_sample\n",
    "\n",
    "    def FV_assoc_classifier(self):\n",
    "        # encoding each feature and then bundling their binding with identification vectors\n",
    "        zipped_x_y_train = np.column_stack((self.x_train, self.y_train))\n",
    "\n",
    "        for e in zipped_x_y_train:\n",
    "            label = int(e[self.feature_len])\n",
    "            sample = np.asarray(e[:self.feature_len])\n",
    "            idx = label - 1\n",
    "            self.classes_assoc_encodings[idx] = np.add(self.classes_assoc_encodings[idx], self.calc_assoc_enc(sample))\n",
    "      \n",
    "        return\n",
    "    \n",
    "    def predict_assoc(self, sample):\n",
    "        assoc_encoded_sample = self.calc_assoc_enc(sample)\n",
    "        predicted_label = self.cosine(assoc_encoded_sample, self.classes_assoc_encodings)\n",
    "        return predicted_label\n",
    "    \n",
    "    def tanh_activation(self, x):\n",
    "        t = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "        return t\n",
    "    \n",
    "    \n",
    "    def normalize_sample(self, sample):\n",
    "        norm_sample = np.zeros(len(sample))\n",
    "        for idx, e in np.ndenumerate(sample):\n",
    "            norm_sample[idx] = (e - self.min_x_train)/(self.max_x_train - self.min_x_train)\n",
    "\n",
    "        return norm_sample\n",
    "    \n",
    "    def calc_kernel_enc(self, sample):\n",
    "        norm_sample = self.normalize_sample(sample)\n",
    "        projected = []\n",
    "        for row in self.kernel:\n",
    "            dot_prod = np.dot(norm_sample, row)\n",
    "            tanh_val = self.tanh_activation(dot_prod)\n",
    "            binarized_val = 1 if tanh_val > 0 else -1\n",
    "            projected.append(binarized_val)\n",
    "\n",
    "        return np.asarray(projected)\n",
    "    \n",
    "    def FV_kernel_classifier(self):\n",
    "        # encoding each feature and then bundling their binding with identification vectors\n",
    "        zipped_x_y_train = np.column_stack((self.x_train, self.y_train))\n",
    "\n",
    "        for e in zipped_x_y_train:\n",
    "            label = int(e[self.feature_len])\n",
    "            sample = np.asarray(e[:self.feature_len])\n",
    "            idx = label - 1\n",
    "            self.classes_kernel_encodings[idx] = np.add(self.classes_kernel_encodings[idx], self.calc_kernel_enc(sample))\n",
    "            \n",
    "        return\n",
    "\n",
    "    def predict_kernel(self, sample):\n",
    "        kernel_encoded_sample = self.calc_kernel_enc(sample)\n",
    "        predicted_label = self.cosine(kernel_encoded_sample, self.classes_kernel_encodings)\n",
    "        return predicted_label\n",
    "    \n",
    "    \n",
    "    def retraining_perm(self):\n",
    "        zipped_x_y_train = np.column_stack((self.x_train, self.y_train))\n",
    "        \n",
    "        for e in zipped_x_y_train:\n",
    "            label = int(e[self.feature_len])\n",
    "            sample = np.asarray(e[:self.feature_len])\n",
    "            encoded_sample = self.calc_perm_enc(sample)\n",
    "            predicted = self.predict_perm(sample)\n",
    "\n",
    "            if predicted == label:\n",
    "                continue\n",
    "            else:\n",
    "                predicted_idx = predicted - 1\n",
    "                label_idx = label - 1\n",
    "                self.classes_perm_encodings[predicted_idx] = np.subtract(self.classes_perm_encodings[predicted_idx], encoded_sample)\n",
    "                self.classes_perm_encodings[label_idx] = np.add(self.classes_perm_encodings[label_idx], encoded_sample)\n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def retraining_assoc(self):\n",
    "        zipped_x_y_train = np.column_stack((self.x_train, self.y_train))\n",
    "\n",
    "        for e in zipped_x_y_train:\n",
    "            label = int(e[self.feature_len])\n",
    "            sample = np.asarray(e[:self.feature_len])\n",
    "            encoded_sample = self.calc_assoc_enc(sample)\n",
    "            predicted = self.predict_assoc(sample)\n",
    "\n",
    "            if predicted == label:\n",
    "                continue\n",
    "            else:\n",
    "                predicted_idx = predicted - 1\n",
    "                label_idx = label - 1\n",
    "                self.classes_assoc_encodings[predicted_idx] = np.subtract(self.classes_assoc_encodings[predicted_idx], encoded_sample)\n",
    "                self.classes_assoc_encodings[label_idx] = np.add(self.classes_assoc_encodings[label_idx], encoded_sample)\n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def retraining_kernel(self):\n",
    "        zipped_x_y_train = np.column_stack((self.x_train, self.y_train))\n",
    "\n",
    "        for e in zipped_x_y_train:\n",
    "            label = int(e[self.feature_len])\n",
    "            sample = np.asarray(e[:self.feature_len])\n",
    "            encoded_sample = self.calc_kernel_enc(sample)\n",
    "            predicted = self.predict_kernel(sample)\n",
    "\n",
    "            if predicted == label:\n",
    "                continue\n",
    "            else:\n",
    "                predicted_idx = predicted - 1\n",
    "                label_idx = label - 1\n",
    "                self.classes_kernel_encodings[predicted_idx] = np.subtract(self.classes_kernel_encodings[predicted_idx], encoded_sample)\n",
    "                self.classes_kernel_encodings[label_idx] = np.add(self.classes_kernel_encodings[label_idx], encoded_sample)\n",
    "                \n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "2f745f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perm_acc(model, num):\n",
    "    test_len = len(y_test[:num])\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(len(x_test[:num])):\n",
    "        pred = model.predict_perm(x_test[i])\n",
    "        label = int(y_test[i])\n",
    "        if pred == label:\n",
    "            counter += 1\n",
    "            \n",
    "    return counter/test_len\n",
    "\n",
    "def get_assoc_acc(model, num):\n",
    "    test_len = len(y_test[:num])\n",
    "    counter = 0\n",
    "    for i in range(len(x_test[:num])):\n",
    "        pred = model.predict_assoc(x_test[i])\n",
    "        label = int(y_test[i])\n",
    "        if pred == label:\n",
    "            counter += 1\n",
    "            \n",
    "    return counter/test_len\n",
    "\n",
    "def get_kernel_acc(model, num):\n",
    "    test_len = len(y_test[:num])\n",
    "    counter = 0\n",
    "    for i in range(len(x_test[:num])):\n",
    "        pred = model.predict_kernel(x_test[i])\n",
    "        label = int(y_test[i])\n",
    "        if pred == label:\n",
    "            counter += 1\n",
    "            \n",
    "    return counter/test_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b966d8ba",
   "metadata": {},
   "source": [
    "### Part 1: Accuracy gained for Permutaion-based, Association-based, and Kernel-based encodings on ISOLET dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "89f1e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdc = hd_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "c7166746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training perm-based classifier...\n",
      "Done! the accuracy on test data for perm-based classifier is: \n",
      "0.7543296985246953\n"
     ]
    }
   ],
   "source": [
    "print('training perm-based classifier...')\n",
    "hdc.FV_perm_classifier()\n",
    "print('Done! the accuracy on test data for perm-based classifier is: ')\n",
    "perm_acc = get_perm_acc(hdc, len(y_test))\n",
    "print(perm_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "4015ac32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training assoc-based classifier...\n",
      "Done! the accuracy on test data for assoc-based classifier is: \n",
      "0.8050032071840924\n"
     ]
    }
   ],
   "source": [
    "print('training assoc-based classifier...')\n",
    "hdc.FV_assoc_classifier()\n",
    "print('Done! the accuracy on test data for assoc-based classifier is: ')\n",
    "assoc_acc = get_assoc_acc(hdc, len(y_test))\n",
    "print(assoc_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "363163ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training kernel-based classifier...\n",
      "Done! the accuracy on test data for kernel-based classifier is: \n",
      "0.7915330339961514\n"
     ]
    }
   ],
   "source": [
    "print('training kernel-based classifier...')\n",
    "hdc.FV_kernel_classifier()\n",
    "print('Done! the accuracy on test data for kernel-based classifier is: ')\n",
    "kernel_acc = get_kernel_acc(hdc, len(y_test))\n",
    "print(kernel_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3332df80",
   "metadata": {},
   "source": [
    "| Model   |      accuracy      | \n",
    "|----------|---------------|\n",
    "| Permutation-based |  %75.43 |\n",
    "| Associacion-based |    %80.50   |\n",
    "| Kernel-based | %79.15 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd4a34a",
   "metadata": {},
   "source": [
    "### Part 2: Accuracy gained for Permutaion-based, Association-based, and Kernel-based encodings on ISOLET dataset with applying retraining on each model \n",
    "##### (here we use 2 pass on the whole training data for each model retraining process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "23f6a7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass no. 1retraining for perm-based encoding...\n",
      "Done! new accuracy on the permutaion-based model:\n",
      "0.7613855035279025\n",
      "pass no. 2retraining for perm-based encoding...\n",
      "Done! new accuracy on the permutaion-based model:\n",
      "0.8088518280949326\n"
     ]
    }
   ],
   "source": [
    "retraining_perm_acc = []\n",
    "for i in range(2):\n",
    "    print('pass no. ' + str(i + 1) + 'retraining for perm-based encoding...')\n",
    "    hdc.retraining_perm()\n",
    "    print('Done! new accuracy on the permutaion-based model:')\n",
    "    t = get_perm_acc(hdc, len(y_test))\n",
    "    print(t)\n",
    "    retraining_perm_acc.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "8e5dd3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass no. 1 retraining for assoc-based encoding...\n",
      "Done! new accuracy on the association-based model:\n",
      "0.8383579217447081\n",
      "pass no. 2 retraining for assoc-based encoding...\n",
      "Done! new accuracy on the association-based model:\n",
      "0.8774855676715844\n"
     ]
    }
   ],
   "source": [
    "retraining_assoc_acc = []\n",
    "for i in range(2):\n",
    "    print('pass no. ' + str(i + 1) + ' retraining for assoc-based encoding...')\n",
    "    hdc.retraining_assoc()\n",
    "    print('Done! new accuracy on the association-based model:')\n",
    "    l = get_assoc_acc(hdc, len(y_test))\n",
    "    print(l)\n",
    "    retraining_assoc_acc.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "01bcd459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass no. 1 retraining for kernel-based encoding...\n",
      "Done! new accuracy on the kernel-based model:\n",
      "0.8678640153944837\n",
      "pass no. 2 retraining for kernel-based encoding...\n",
      "Done! new accuracy on the kernel-based model:\n",
      "0.8717126363053239\n"
     ]
    }
   ],
   "source": [
    "retraining_kernel_acc = []\n",
    "for i in range(2):\n",
    "    print('pass no. ' + str(i + 1) + ' retraining for kernel-based encoding...')\n",
    "    hdc.retraining_kernel()\n",
    "    print('Done! new accuracy on the kernel-based model:')\n",
    "    m = get_kernel_acc(hdc, len(y_test))\n",
    "    print(m)\n",
    "    retraining_kernel_acc.append(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05ffe33",
   "metadata": {},
   "source": [
    "| Model   |      1 pass retraining accuracy    | 2 pass retraining accuracy |\n",
    "|----------|---------------|---------------|\n",
    "| Permutation-based |  %76.13 |%80.88 |\n",
    "| Associacion-based |    %83.83   |%87.74 |\n",
    "| Kernel-based | %86.78 |%87.17 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053a4c3a",
   "metadata": {},
   "source": [
    "### Part 3: Accuracy gained for Permutaion-based, Association-based, and Kernel-based encodings on ISOLET dataset with applying adaptive single-pass training\n",
    "##### (here we use gamma = 0.5 as learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "b4843077",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_hdc = hd_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "9d4213ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive single-pass perm-based classifier...\n",
      "Done! the accuracy on test data for adaptive perm-based classifier is: \n",
      "0.7613855035279025\n"
     ]
    }
   ],
   "source": [
    "print('Adaptive single-pass perm-based classifier...')\n",
    "adaptive_hdc.FV_adaptive_perm_classifier()\n",
    "print('Done! the accuracy on test data for adaptive perm-based classifier is: ')\n",
    "adaptive_perm_acc = get_perm_acc(adaptive_hdc, len(y_test))\n",
    "print(adaptive_perm_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "39efbe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive single-pass assoc-based classifier...\n",
      "Done! the accuracy on test data for adaptive assoc-based classifier is: \n",
      "0.7171263630532393\n"
     ]
    }
   ],
   "source": [
    "print('Adaptive single-pass assoc-based classifier...')\n",
    "adaptive_hdc.FV_adaptive_assoc_classifier()\n",
    "print('Done! the accuracy on test data for adaptive assoc-based classifier is: ')\n",
    "adaptive_assoc_acc = get_assoc_acc(adaptive_hdc, len(y_test))\n",
    "print(adaptive_assoc_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "894d3d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive single-pass kernel-based classifier...\n",
      "Done! the accuracy on test data for adaptive kernel-based classifier is: \n",
      "0.03848620910840282\n"
     ]
    }
   ],
   "source": [
    "print('Adaptive single-pass kernel-based classifier...')\n",
    "t = hd_classifier()\n",
    "t.FV_adaptive_kernel_classifier()\n",
    "print('Done! the accuracy on test data for adaptive kernel-based classifier is: ')\n",
    "adaptive_kernel_acc = get_kernel_acc(t, len(y_test))\n",
    "print(adaptive_kernel_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bcd68f",
   "metadata": {},
   "source": [
    "#### Well, I checked the code couple of times but don't know what is happening with kernel-based adaptive single-pass training :/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c10a6cf",
   "metadata": {},
   "source": [
    "### Part 4: Accuracy gained for Permutaion-based, Association-based, and Kernel-based encodings on ISOLET dataset with different dimentionalities\n",
    "##### (since the retraining method takes about double time than the single pass one, and also the effect of dimentionality can be shown in both cases, we use single pass training here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "91f89309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for dim= 1000 is:\n",
      "0.21808851828094933\n",
      "accuracy for dim= 2000 is:\n",
      "0.7280307889672867\n",
      "accuracy for dim= 3000 is:\n",
      "0.6459268762026941\n",
      "accuracy for dim= 4000 is:\n",
      "0.7447081462475946\n",
      "accuracy for dim= 5000 is:\n",
      "0.7491982039769083\n"
     ]
    }
   ],
   "source": [
    "dim_test_perm = []\n",
    "models = [hd_classifier((i+1)*1000) for i in range(5)]\n",
    "for i in range(len(models)):\n",
    "    model = models[i]\n",
    "    model.FV_perm_classifier()\n",
    "    acc = get_perm_acc(model, len(y_test))\n",
    "    print('accuracy for dim= ' + str((i+1)*1000) + ' is:')\n",
    "    print(acc)\n",
    "    dim_test_perm.append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf9de2",
   "metadata": {},
   "source": [
    "#### As we can see, the accuracy enhances with increasing the dimentionality\n",
    "| Dim   | accuracy |\n",
    "|----------|---------------|\n",
    "| 1000 | %21.81 |\n",
    "| 2000 | %72.80 |\n",
    "| 3000 | %64.59 |\n",
    "| 4000 | %74.47 |\n",
    "| 5000 | %74.91 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b668f3",
   "metadata": {},
   "source": [
    "#### Part 5: The effect of batch size cannot be seen here since we are just accumulating the weights instead of adding them one by one. Actually time constraint is another reason this result is not available"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
